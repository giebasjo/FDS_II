---
title: "Homework 1"
author: "Jordan Giebas"
date: "Due Wednesday, November 1 at 5:30 PM"
output: pdf_document
---

\large

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\vspace{.2in}

__Part 1:__

Reconsider the data set that was presented in the final exam for Mini 1.
Create a new data frame that gives the day-to-day changes in all of the
rates. Run a principal components analysis on these vectors. Is PCA
effective in reducing the dimensionality of the rate change vectors?
Try it with and without scaling the variables first
and describe how the results change.


```{r, Question1}
library(quantmod)
library(ggplot2)
library(plyr)

## Get the data from the final exam
######################################################################
fileheader = read.table("commercial_paper_rates.csv",sep=",",nrows=6,stringsAsFactors=FALSE)
cprfullnames = as.character(c("Time.Period",fileheader[1,-1]))
cprdata = read.table("commercial_paper_rates.csv",sep=",", header=T,
                      skip=5,stringsAsFactors = FALSE,
                      na.strings=c("ND","NA"))

cprdata$Time.Period = as.Date(cprdata$Time.Period, format="%Y-%m-%d")
fulldata = cprdata
#fulldata = cprdata[complete.cases(cprdata),]
######################################################################

dates = fulldata$Time.Period
dates = dates[2:length(dates)]

data = fulldata[,c(2:ncol(fulldata))]
data = colwise(diff)(data)

df = cbind(dates, data)
df_dates = df[,1]
df_data = df[,c(2:ncol(df))]

df_data = df_data[complete.cases(df_data),]

df_pca_noscale = princomp(df_data)
summary(df_pca_noscale)

df_pca_scale = princomp(df_data, cor=TRUE)
summary(df_pca_scale)
```

Unscaled case: The PCA is definitely reasonable; however, it's not great. We see approximately 90% of the variability captured within 12 of the Principal Components, i.e. half the predictors that were original. Further, 95% of the variability is captured in the first 16 pcs, i.e. 2/3 of the predictors. In this sense, the unscaled PCA is rather useful depending on how willing the user is to sacrifice capturing some of the variability.

Scaled case: The scaled case is quite similar to the unscaled case, perhaps it is a little worse in the sense that the scaled case requires 15 principal components to capture 90% of the variability rather than 12. It's sensible that the scaled case is highly similar to the unscaled since the variables are rates and are on the same scale.

\vspace{.2in}

__Part 2:__

As described in lecture, one approach to the time series dimension
reduction situation we were facing would be to first smooth the time
series, and then use these smoothed time series as the input to a
dimension reduction algorithm. We will try this here.

In particular, smooth each time series using `loess()`. I will leave the
choice of the smoothing parameter up to you. After smoothing, you should
use the `predict()` function to evaluate the fitted model on a regular
grid of $x$ values. These smoothed time series are what should be utilized
in the dimension reduction. Use Isomap. Explore the first two-dimensions
to see if there is meaningful low-dimensional structure in the plot.

__Comment:__ If you watched lecture, this will make a lot more sense. Be
sure to watch the lecture prior to attempting this.

```{r}
library(vegan)
library(ggplot2)

## Data from lecture, smoothing the data using loess
stocksample = read.table("stocksample.txt", header=T, sep="\t", comment.char="")
smooth_df = apply(t(stocksample)[5:34,], 2, function(x) { return (predict(loess(x~c(1:30))))})

## Scaling the data
ss_scaled = apply(smooth_df, 2, scale)

## Distance/Isomap 
iso_out = isomap(dist(t(ss_scaled)), k=5)
stocksample = cbind(stocksample, iso_out$points[,1:5])

plt = ggplot(stocksample,aes(x=stocksample[,35],y=stocksample[,36], color=sector)) + 
  geom_point() +
  labs(x=expression(U[1]), y=expression(U[2]), color="Sector")

plt

```

After running isomap and looking at the first two dimensions, it's clear that there is a centric pattern across the variety of sectors. Further, there is some apparent clustering when $U1 \approx 5$. The dimension reduction is rather efficient, for there is a clear pattern represented in this two dimensional space. Whether this is because of similarities across sectors versus across time is a different question: one could test this by sampling a proportion of companies falling in some neighborhood of $U1 = 5$ and looking at comparing data of different sectors over time. If there is a similar trend over the time horizon, this clustering effect is likely due in part because of the time than the sector. 



